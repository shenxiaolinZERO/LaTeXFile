
\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{epstopdf}
\usepackage{amsmath}
\usepackage{xcolor}

\title{The BTL Model}
\author{Xiaolin Shen}

\begin{document}

\maketitle


\section{The Standard BTL Model}

Suppose that there are $K$ underlying aspects, the latent preference vector for each user is denoted as $ u \in  R+ ^K$ , and $\sum_k u_k = 1$ , latent item feature vector $ i, j \in R + ^K$.

Base on the standard BTL model:

\begin{align*}
	p(i \succ j |u)=\frac{u^{T}i}{u^{T}i+ u^{T}j} \\
	p(i \prec j |u)=1-p(i \succ j |u)
\end{align*}

We use the maximal likelihood estimation. The likelihood function can be written as follows. The model parameters are denoted as $ \Theta = \{ v \in V ,u \in U ) \}$, $\succ_d $ represents a pairwise ranking observation in a session $d$.
\begin{align}
L(\Theta)
= \Pi_{d \in D} \Pi_{i \succ_d j}\frac{\sum_{k=1}^{K} u_k i_k}{\sum_{k=1}^{K} u_k i_k+ \sum_{k=1}^{K} u_k j_k}
\end{align}

Thus, the log likelihood and its lower bound is:
\begin{align}
l(\Theta) &= \log \; L(\Theta) \\
& =\sum_{u} \sum_{d\in D_u} \sum_{i \succ_d j} \log \;  \frac{\sum_k u_k i_k}{\sum_k u_k i_k+ \sum_k u_k j_k}\\ \nonumber
& \geq \sum_{u} \sum_{d\in D_u} \sum_{i \succ_d j}[1- \frac{\sum_k u_k i_k+ \sum_k u_k j_k}{\sum_k u_k^t i_k^t+ \sum_k u_k^t j_k^t}+ \log \frac{\sum_k u_k i_k}{\sum_k u_k^t i_k^t+ \sum_k u_k^t j_k^t}]
\end{align}
where we apply 
\begin{equation*}
\ln \frac{y}{x} \geq 1- \frac{x}{y}
\end{equation*}

To maximize the lower-bound in $Equ.3$ we employ conjugate gradient method. 

We first fix all $v$s, and apply  $ \log \sum_k u_k i_k \geq \frac{\sum_k i_k \log u_k}{\sum_k i_k} + \log \sum_k i_k,\forall i_k \geq 0.$  denote $ c^t(i, j, d)=\sum_k u_k^t i_k^t+ \sum_k u_k^t j_k^t  $ in the $t$-th round of the iteration, with respect to $\sum_k u_k=1$,we have

\begin{align}
\frac{\partial \;l(\Theta) }{\partial \;u_k}
 &=\sum_{d\in D_u} \sum_{i \succ_d j} (\frac{i_k}{u_k \sum_k i_k}-\frac{i_k+j_k}{c^t(i, j, d)}) =0
\end{align}




\subsection{For u}
By setting the partial derivative of $\frac{l(\Theta)}{\partial u_k}=0$, we have:

\begin{align}
\frac{\partial \;l(\Theta) }{\partial \;u_k}
 &= \Sigma_{d \in D} \Sigma_{w\in W^d, v\in L^d}(\frac{\sum_k w_k}{\sum_k u_k w_k}-\frac{\sum_k(w_k+v_k)}{\sum_k u_k^t(w_k^t +v_k^t)}) =0
\end{align}
$$ \frac{\sum_k w_k}{\sum_k u_k w_k}=\frac{\sum_k(w_k+v_k)}{\sum_k u_k^t(w_k^t +v_k^t)}$$
$$ \sum_k u_k w_k =\frac{ \sum_k w_k \sum_k u_k^t(w_k^t +v_k^t)}{\sum_k(w_k+v_k)} $$
$$ u_k w_k  = \frac{\sum_k w_k \sum_k u_k^t(w_k^t +v_k^t)}{\sum_k(w_k+v_k)} -\sum_{k' \neq k} u_{k'} w_{k'} $$

$$ u_k = \frac{\sum_k u_k^t(w_k^t +v_k^t)}{\sum_k(w_k+v_k)} - \frac{\sum_{k' \neq k} u_{k'} w_{k'}}{w_k}$$

Eventually,
$$ u_k = \Sigma_{d \in D} \Sigma_{w\in W^d, v\in L^d}(\frac{\sum_k u_k^t(w_k^t +v_k^t)}{\sum_k(w_k+v_k)} - \frac{\sum_{k' \neq k} u_{k'} w_{k'}}{w_k})$$
Note that $\sum_k u_k=1$

\subsection{For v}

As for winner:\\
By setting the partial derivative of $\frac{l(\Theta)}{\partial w_k}=0$, we have:
\begin{align}
\frac{\partial \;l(\Theta)}{\partial \;w_k}
&=  \Sigma_{d \in W(v)} \Sigma_{v'\in L_d}(\frac{\sum_k u_k}{w_k}-\frac{\sum_k u_k}{\sum_k u_k^t(w_k^t +v_k^{'t})})=0
\end{align}
$$ w_k =\Sigma_{d \in W(v)} \Sigma_{v'\in L_d} \sum_k u_k^t(w_k^t +v_k^{'t}) $$

As for loser:\\
By setting the partial derivative of $\frac{l(\Theta)}{\partial v_k}=0$, we have:
\begin{align}
\frac{\partial \;l(\Theta)}{\partial \;v_k}
&=  \Sigma_{d \in L(v)} \Sigma_{v'\in W_d}(-\frac{\sum_k u_k}{\sum_k u_k^t(v_k^{'t} +v_k^t)})=0
\end{align}
$$ v_k =  $$
Eventually,
$$ v_k = \Sigma_{d \in W(v)} \Sigma_{v'\in L_d} \sum_k u_k^t(w_k^t +v_k^{'t}) +\Sigma_{d \in L(v)} \Sigma_{v'\in W_d}()$$

\end{document}
